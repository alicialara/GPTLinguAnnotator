# Technical Documentation of GPTLinguAnnotator Framework

## Introduction and Overview

GPTLinguAnnotator Framework is a specialized platform designed for the classification and analysis of chromatic terms in different linguistic corpora. It uses large language models (LLMs) to semantically categorize instances of color-related words (`black`, `blacken`, `white`, `whiten`, `red`, `redden`, `pink`, `pinken`) in English texts.

The system architecture is designed as a modular pipeline that spans from data ingestion to result evaluation and report generation, with specialized components for each stage of the process.

## System Architecture

### Directory Structure

```
semantic_analysis/
├── config/             # Global system configuration
├── data/               # Input data (annotated corpora)
├── docs/               # Documentation and results analysis
├── outputs/            # Results generated by the system
├── src/                # Core framework
│   ├── prompts/        # Prompt configurations by word
│   ├── data_processor.py  # Data processing
│   ├── experiment.py      # Experiment management
│   ├── model.py           # LLM interface
│   ├── annotation_saver.py # Annotation saving
│   └── prompts.py         # Prompt system
├── main.py             # Main entry point
├── evaluation.py       # Result evaluation
├── comparative_analysis.py # Comparative analysis
└── run_all_words.py    # Batch processing
```

### Main Components

1. **Framework Core (`src/`):**
   - `data_processor.py`: Handles reading and processing input data
   - `model.py`: Manages communication with different LLMs
   - `experiment.py`: Implements experiment execution logic
   - `prompts/`: Contains specific prompts for each analyzed word

2. **Configuration (`config/`):**
   - Defines available models, semantic dictionaries, and experiment configurations
   - Establishes semantic categories for each chromatic term
   - Manages file and directory paths

3. **Evaluation and Analysis:**
   - `evaluation.py`: Performance metrics and visualizations
   - `comparative_analysis.py`: Comparisons between models and corpora
   - `visualization.py`: Advanced visualization generation

4. **Interfaces:**
   - `main.py`: Main CLI for running experiments
   - Auxiliary scripts for specific tasks (`run_all_words.py`, etc.)

## System Workflow

### 1. Data Ingestion and Processing

**Implementation:** `src/data_processor.py`

The system accepts data in Excel format with two main input schemas:
- **New Format:** A `Concordance` column containing the full text
- **Legacy Format:** Columns `Left`, `KWIC` (keyword in context), and `Right`

Key functionalities:
- Automatic detection of input format
- Conversion to a unified internal format
- Extraction of the target chromatic term
- Text preprocessing for analysis

```python
# Example of data reading
df = read_excel_data(experiment.file_path, experiment.sheet_name)
```

### 2. Prompt System

**Implementation:** `src/prompts/` and `src/prompts.py`

The framework uses an advanced prompt system specific to each chromatic term, with:

- **Modular structure:** Each term has its own prompt configuration file
- **Semantic dictionaries:** Formal definitions for each semantic category
- **Specific instructions:** Detailed guidelines for the model on how to classify each case

The main interface `get_prompt_config()` loads the appropriate configuration based on:
- The term to analyze (`black`, `whiten`, etc.)
- The semantic dictionary to use (default: "GENERIC")

```python
# Loading prompt configuration
messages = get_prompt_config(experiment.word, dictionary_key)
```

### 3. Language Model Integration

**Implementation:** `src/model.py`

The system supports multiple LLM backends:
- **OpenAI** (GPT-4o-mini): Via official OpenAI API
- **Anthropic** (Claude 3.7 Sonnet): Through local LLM Studio
- **Gemini** (optional): Using Google Generative AI

The component provides:
- Unified API client initialization
- Message formatting for each model
- Response processing
- Semantic category extraction

```python
# Initialization and model query
client = initialize_client(model)
response = get_model_response(client, model, text, word, messages)
category = extract_category(response, word)
```

### 4. Experiment Execution

**Implementation:** `src/experiment.py`

Coordinates the complete execution of semantic analysis experiments:
- Loads specific configurations for each experiment
- Processes each instance through the selected model
- Extracts and stores the assigned semantic categories
- Generates preliminary performance metrics

```python
# Running a complete experiment
results = run_experiment(experiment, dictionary_key, model, only_human_annotated)
```

### 5. Evaluation and Analysis

**Implementation:** `evaluation.py` and `comparative_analysis.py`

Complete evaluation system with:
- **Detailed metrics:** Accuracy, Cohen's Kappa, precision, recall, F1-score
- **Analysis by category:** Performance breakdown by semantic category
- **Visualizations:** Confusion matrices, distributions, comparative graphs
- **Reports:** Report generation in text, Excel, HTML, and PDF formats

```python
# Result evaluation
evaluation = evaluate_results(df, human_column="HUMAN", results_column="GPT_CATEGORY")
```

## Advanced Technical Features

### Output Organization System

The framework implements a hierarchical structure for result organization:
```
outputs/
├── {word}/            # Specific directory for each word
│   ├── {model}/       # Subdirectory for each model
│   │   ├── {corpus}_annotations_{dictionary}.xlsx  # Direct results
│   │   ├── {corpus}_annotations_{dictionary}.json  # JSON format
│   │   ├── {corpus}_annotations_{dictionary}_viewer.html  # HTML viewer
│   │   └── evaluation_report_{word}_{corpus}.txt  # Evaluation report
```

This structure facilitates:
- Orderly management of multiple experiments
- Comparisons between models and corpora
- Result traceability

### Framework Extensibility

The system is designed to be extensible in several dimensions:

1. **New chromatic terms:**
   - Add configuration file in `src/prompts/`
   - Define semantic categories in `config/config.py`
   - Update mapping in `WORD_CATEGORIES` and `WORD_DICTIONARIES`

2. **New models:**
   - Register in `AVAILABLE_MODELS` in `config/config.py`
   - Implement initialization and calling in `model.py`

3. **New corpora:**
   - Add `ExperimentConfig` entries in `EXPERIMENTS`
   - Maintain compatible Excel file format

### Performance Considerations

- **API costs:** The system is optimized to minimize the number of calls to external APIs
- **Parallelization:** The current implementation is sequential, but allows adaptation for parallel processing
- **Persistence:** Intermediate results stored to allow resumption of experiments

## Command Line Interface

The framework provides a complete CLI interface through several entry points:

### Main Execution

```bash
python semantic_analysis/main.py [--model MODEL] [--corpus CORPUS] [--word WORD] [--only-human]
```

Main options:
- `--model`: Model to use (gpt-4o-mini, claude-3.7-sonnet)
- `--corpus`: Specific corpus to process
- `--word`: Specific word to analyze
- `--only-human`: Process only instances with human annotations

### Evaluation

```bash
python semantic_analysis/evaluation.py --corpus CORPUS [--model MODEL] [--html]
```

### Comparative Analysis

```bash
python semantic_analysis/comparative_analysis.py [--html] [--debug]
```

## Extensions and Additional Components

### Advanced Visualization

The `visualization.py` component provides:
- Interactive HTML visualization generation
- Conversion to PDF for reports
- Comparative dashboards

### Annotation Saving

The `annotation_saver.py` module implements:
- Result storage in multiple formats
- Interactive HTML viewer generation
- JSON serialization for interoperability

## Conclusions and Technical Considerations

The Semantic Analysis Framework represents a sophisticated and modular platform for the semantic classification of chromatic terms. Its main technical advantages are:

1. **Modular architecture:** Clearly separated components with specific responsibilities
2. **Extensibility:** Support for multiple models, terms, and corpora
3. **Comprehensive evaluation:** Complete system of metrics and visualizations
4. **Flexible interface:** Multiple entry points and configuration options

Current limitations and areas for future development:
1. **Parallelization:** Implement parallel processing to improve performance
2. **Web interface:** Develop web frontend for more intuitive interaction
3. **Result caching:** Optimize storage of LLM responses
4. **Incremental adaptation:** Mechanisms for continuous training based on feedback 